{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:01<00:57,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 : tensor(0.5312, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.13034296236922885 0.0\n",
      "beta_r 0.09713763504097611 0.0\n",
      "alpha_u 0.20908475755163664 0.0\n",
      "beta_u 0.20618544456611615 0.0\n",
      "gamma 0.12650661877927707 0.0\n",
      "theta 0.03156057554525556 0.0\n",
      "_lambda 2.5010146438436625 0.0\n",
      "d 2.9998455272874933 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [00:05<00:40,  2.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 11 : tensor(-0.5151, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.2755457216326264 0.0\n",
      "beta_r 0.15666622983359144 0.0\n",
      "alpha_u 0.2505712943278626 0.0\n",
      "beta_u 0.2437426336951329 0.0\n",
      "gamma 0.24119591149661185 0.0\n",
      "theta 0.08704281206781793 0.0\n",
      "_lambda 2.5068783479339083 0.0\n",
      "d 2.9996222845135163 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 22/100 [00:10<00:37,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 21 : tensor(-3.5354, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.38014186702567976 0.0\n",
      "beta_r 0.28223742364525106 0.0\n",
      "alpha_u 0.293927870245602 0.0\n",
      "beta_u 0.3093438747473389 0.0\n",
      "gamma 0.34470448940332205 0.0\n",
      "theta 0.15292381623699128 0.0\n",
      "_lambda 2.5128223886988286 0.0\n",
      "d 2.9996374919437754 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 32/100 [00:15<00:32,  2.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 31 : tensor(-6.1632, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.44452635873522367 0.0\n",
      "beta_r 0.4094493647944498 0.0\n",
      "alpha_u 0.33830065470696535 0.0\n",
      "beta_u 0.4115795498008545 0.0\n",
      "gamma 0.4359202721244157 0.0\n",
      "theta 0.23284273281958953 0.0\n",
      "_lambda 2.5189768255404474 0.0\n",
      "d 2.9993539981147177 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 42/100 [00:20<00:27,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 41 : tensor(-14.1165, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.47278282438475866 0.0\n",
      "beta_r 0.5406348963765129 0.0\n",
      "alpha_u 0.3740966959115111 0.0\n",
      "beta_u 0.504879354730365 0.0\n",
      "gamma 0.5379279601116264 0.0\n",
      "theta 0.3342153855976852 0.0\n",
      "_lambda 2.5264959046622453 0.0\n",
      "d 2.9982739450839078 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 52/100 [00:24<00:22,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 51 : tensor(-31.7368, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.4868561135394113 0.0\n",
      "beta_r 0.6622113554385666 0.0\n",
      "alpha_u 0.4028744401581622 0.0\n",
      "beta_u 0.5673336195336528 0.0\n",
      "gamma 0.6427861808804568 0.0\n",
      "theta 0.43907360636651804 0.0\n",
      "_lambda 2.5346354173072094 0.0\n",
      "d 2.990249472497555 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 62/100 [00:29<00:18,  2.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 61 : tensor(-57.1987, dtype=torch.float64, grad_fn=<LogBackward0>)\n",
      "alpha_r 0.49335640712965617 0.0\n",
      "beta_r 0.8003874334251756 0.0\n",
      "alpha_u 0.42421570155379756 0.0\n",
      "beta_u 0.5978380912547311 0.0\n",
      "gamma 0.7603116598986736 0.0\n",
      "theta 0.5565990853847352 0.0\n",
      "_lambda 2.5434302249170577 0.0\n",
      "d 2.9631053796011346 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 64/100 [00:30<00:17,  2.08it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 96\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# EM_sampler=EM(5,0.2, 0.2, 6.0, 0.6, 0.4, 0.1, 0.02, 2.5,rfilename=\"./Bayes_temp/r.npy\")\u001b[39;00m\n\u001b[0;32m     95\u001b[0m EM_sampler\u001b[38;5;241m=\u001b[39mEM(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.2\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.02\u001b[39m, \u001b[38;5;241m2.5\u001b[39m,rfilename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./r.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mEM_sampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28mprint\u001b[39m(EM_sampler\u001b[38;5;241m.\u001b[39mparameters)\n",
      "Cell \u001b[1;32mIn[3], line 78\u001b[0m, in \u001b[0;36mEM.optimize\u001b[1;34m(self, num_steps, num_steps_hidden)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_steps)):\n\u001b[0;32m     77\u001b[0m     n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m---> 78\u001b[0m     weights,epsilon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_sampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m#print(_)\u001b[39;00m\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m __ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps_hidden):\n",
      "Cell \u001b[1;32mIn[3], line 58\u001b[0m, in \u001b[0;36mEM.call_sampler\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m     56\u001b[0m params\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_r\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_r\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_u\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_u\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lambda\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m     57\u001b[0m sampler \u001b[38;5;241m=\u001b[39m TEST_SAMPLER(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT, params)\n\u001b[1;32m---> 58\u001b[0m samples, weights\u001b[38;5;241m=\u001b[39m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mexp_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lambda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m#print(\"Weights&Samples\",weights,samples)\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(weights),torch\u001b[38;5;241m.\u001b[39mtensor(samples)\n",
      "File \u001b[1;32mc:\\Users\\cyx20\\Desktop\\大学\\2023-2024春 影视中的英美文化\\Bayes_temp\\sampler.py:58\u001b[0m, in \u001b[0;36mTEST_SAMPLER.sample\u001b[1;34m(self, sample_num, r, exp_scale, resample_thre)\u001b[0m\n\u001b[0;32m     54\u001b[0m w\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_u\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_u\u001b[38;5;241m*\u001b[39mu_past\u001b[38;5;241m+\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtheta\u001b[38;5;241m*\u001b[39m(eps_past\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0\u001b[39m))\u001b[38;5;241m*\u001b[39m(eps_past\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     57\u001b[0m eps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy(eps_past, rr, w, exp_scale)\n\u001b[1;32m---> 58\u001b[0m log_weights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_likelihood_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43mrr\u001b[49m\u001b[43m,\u001b[49m\u001b[43meps_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43mr_past\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_policy_density(eps, rr, w, exp_scale)\n\u001b[0;32m     59\u001b[0m r_past\u001b[38;5;241m=\u001b[39mrr\n\u001b[0;32m     60\u001b[0m samples[:,i]\u001b[38;5;241m=\u001b[39meps\n",
      "File \u001b[1;32mc:\\Users\\cyx20\\Desktop\\大学\\2023-2024春 影视中的英美文化\\Bayes_temp\\sampler.py:32\u001b[0m, in \u001b[0;36mTEST_SAMPLER.log_likelihood_update\u001b[1;34m(self, epsilon, r, epsilon_past, r_past)\u001b[0m\n\u001b[0;32m     27\u001b[0m eta\u001b[38;5;241m=\u001b[39m(r\u001b[38;5;241m-\u001b[39mepsilon\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_r)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta_r\u001b[38;5;241m-\u001b[39mw\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#eta=np.maximum(eta,1e-7)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#print(f\"eps:{epsilon[:10]}\\n eps_past:{epsilon_past[:10]}\\n r:{r} r_past:{r_past} etamin:{eta.min()}\\n w:{w[:10]}, nu:{nu[:10]}\\n eta:{eta[:10]}\\n\")\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#print(eta.min())\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#eta=(eta>=0)*eta+(eta<=0)*1e-7\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m eta\u001b[38;5;241m.\u001b[39mmin()\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m#eta should follow exponential distribution\u001b[39;00m\n\u001b[0;32m     34\u001b[0m logp_exp\u001b[38;5;241m=\u001b[39mexpon\u001b[38;5;241m.\u001b[39mlogpdf(eta, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lambda)\n\u001b[0;32m     35\u001b[0m logp_t\u001b[38;5;241m=\u001b[39mt\u001b[38;5;241m.\u001b[39mlogpdf(nu,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.distributions import StudentT, Exponential\n",
    "from sampler import TEST_SAMPLER\n",
    "from tqdm import tqdm\n",
    "\n",
    "#print('\\n'*10)\n",
    "class EM:\n",
    "    def __init__(self, T=100, _alpha_r=0, _beta_r=1, _d=1, _alpha_u=0, _beta_u=1, _gamma=1, _theta=0, __lambda=1,rfilename=\"./r.npy\"):\n",
    "        self.alpha_r = torch.tensor(_alpha_r, dtype=torch.float64, requires_grad=True)\n",
    "        self.beta_r = torch.tensor(_beta_r, dtype=torch.float64, requires_grad=True)\n",
    "        self.alpha_u = torch.tensor(_alpha_u, dtype=torch.float64, requires_grad=True)\n",
    "        self.beta_u = torch.tensor(_beta_u, dtype=torch.float64, requires_grad=True)\n",
    "        self.gamma = torch.tensor(_gamma, dtype=torch.float64, requires_grad=True)\n",
    "        self.theta = torch.tensor(_theta, dtype=torch.float64, requires_grad=True)\n",
    "        self._lambda = torch.tensor(__lambda, dtype=torch.float64, requires_grad=True)\n",
    "        self.d = torch.tensor(_d, dtype=torch.float64, requires_grad=True)\n",
    "        self.parameters=[self.alpha_r, self.beta_r, self.alpha_u, self.beta_u, self.gamma, self.theta, self._lambda, self.d]\n",
    "        self.names=[\"alpha_r\", \"beta_r\", \"alpha_u\", \"beta_u\", \"gamma\", \"theta\", \"_lambda\", \"d\"]\n",
    "        self.T=T\n",
    "        self.r=np.load(rfilename)\n",
    "        self.r=torch.tensor(self.r).reshape(1,-1)\n",
    "        assert self.T ==self.r.shape[1]\n",
    "    def singularlikelihood(self,epsilon):\n",
    "        ''' Compute log joint likelihood of l=log p(eps_1,...,eps_T,r_1,...r_T)'''\n",
    "        #Input:  epsilon  (n,T)\n",
    "        #Output: log_prob (n,)\n",
    "\n",
    "        n,T=epsilon.shape\n",
    "        prior=0 #do we need prior on r_0, eps_0?\n",
    "\n",
    "        ''' Calculate u, w, nu and eta'''\n",
    "        epsilon_shift=torch.zeros(n,T)\n",
    "        epsilon_shift[:,1:]=epsilon[:,:T-1] #eps_{t-1}\n",
    "        u=(self.r-epsilon-self.alpha_r)/self.beta_r\n",
    "        u_shift=torch.zeros_like(u) #u_{t-1}\n",
    "        u_shift[:,1:]=u[:,:T-1]\n",
    "        w=self.alpha_u+self.beta_u*u_shift+(self.gamma+self.theta*(epsilon_shift<0))*(epsilon_shift**2)\n",
    "        nu=torch.sqrt(self.beta_r/(self.r-epsilon-self.alpha_r))*epsilon\n",
    "        eta=(self.r-epsilon-self.alpha_r)/self.beta_r-w\n",
    "        eta=torch.maximum(torch.zeros_like(eta),eta)+1e-6\n",
    "        #print(eta)\n",
    "        assert eta.min()>=0 #eta should follow exponential distribution\n",
    "\n",
    "        ''' Calculate each component of the log-likelihood'''\n",
    "        t_distr=StudentT(self.d)\n",
    "        exp_distr=Exponential(self._lambda) #remains to be checked: lambda or 1/lambda\n",
    "        logp_t=t_distr.log_prob(nu)\n",
    "        logp_exp=exp_distr.log_prob(eta)\n",
    "        log_joint=torch.sum(logp_t+logp_exp -0.5*(torch.log(self.beta_r)+torch.log(self.r-epsilon-self.alpha_r)), dim=-1)+prior\n",
    "\n",
    "\n",
    "        return log_joint\n",
    "    def call_sampler(self,n):\n",
    "        #remember to add .item() at final version\n",
    "        params=(self.alpha_r.item(), self.beta_r.item(), self.d.item(), self.alpha_u.item(), self.beta_u.item(), self.gamma.item(), self.theta.item(), self._lambda.item())\n",
    "        sampler = TEST_SAMPLER(self.T, params)\n",
    "        samples, weights=sampler.sample(n, np.array(self.r.T),exp_scale=1/self._lambda.item())\n",
    "        #print(\"Weights&Samples\",weights,samples)\n",
    "        return torch.tensor(weights),torch.tensor(samples)\n",
    "    def upd_param(self,lr=1e-4):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                param += lr * param.grad\n",
    "                param.grad.zero_()\n",
    "        #print(self.parameters)\n",
    "    def log_total(self,weights,epsilon):\n",
    "        likelihood=self.singularlikelihood(epsilon)\n",
    "        #print(\"Sing l\",likelihood)\n",
    "        normed=likelihood+torch.log(weights) #item for removing normalization from gradients\n",
    "        return torch.log(torch.sum(torch.exp(normed)))\n",
    "    def state_EM(self):\n",
    "        for i,param in enumerate(self.parameters):\n",
    "            print(self.names[i],param.item(),param.grad.item())\n",
    "    def optimize(self,num_steps=10,num_steps_hidden=50):\n",
    "        for _ in tqdm(range(num_steps)):\n",
    "            n=10000\n",
    "            weights,epsilon=self.call_sampler(n)\n",
    "            #print(_)\n",
    "            for __ in range(num_steps_hidden):\n",
    "                likelihood=self.log_total(weights,epsilon)\n",
    "                #print(\"Likelihood:\",likelihood)\n",
    "                likelihood.backward()\n",
    "                self.upd_param(lr=1e-5)\n",
    "            if _%10==1:\n",
    "                print(\"step\",_,\":\",likelihood)\n",
    "                self.state_EM()\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# EM_sampler=EM(5,0.2, 0.2, 6.0, 0.6, 0.4, 0.1, 0.02, 2.5,rfilename=\"./Bayes_temp/r.npy\")\n",
    "EM_sampler=EM(5,0.1, 0.1, 3.0, 0.2, 0.2, 0.1, 0.02, 2.5,rfilename=\"./r.npy\")\n",
    "EM_sampler.optimize(num_steps=100)\n",
    "print(EM_sampler.parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1532]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.0002]], grad_fn=<AddBackward0>),\n",
       " tensor([[0.0877]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VIScaler(nn.Module):\n",
    "    def __init__(self, hidden_size=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(3, hidden_size)\n",
    "        self.fc21 = nn.Linear(hidden_size, 1)\n",
    "        self.fc22 = nn.Linear(hidden_size, 1)\n",
    "        self.fc23 = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x1 = self.fc21(x)\n",
    "        x2 = self.fc22(x)\n",
    "        x3 = self.fc23(x)\n",
    "        x1 = 1e-2+torch.sigmoid(x1)\n",
    "        x2 = 1e-4+0.4*torch.sigmoid(x2)\n",
    "        x3 = 1e-4+0.1*torch.sigmoid(x3)\n",
    "        return x1,x2,x3\n",
    "    \n",
    "model=VIScaler(hidden_size=16)\n",
    "model=torch.load(\"VIScaler_test1_153_loss_94.6197280883789.pth\")\n",
    "\n",
    "input=torch.tensor([[2,2,4.]])\n",
    "\n",
    "model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class EM:\n",
    "    def __init__(self, T, _alpha_r=0, _beta_r=1, _d=1, _alpha_u=0, _beta_u=1, _gamma=1, _theta=0, __lambda=1):\n",
    "        self.alpha_r = torch.tensor(_alpha_r, dtype=torch.float32, requires_grad=True)\n",
    "        self.beta_r = torch.tensor(_beta_r, dtype=torch.float32, requires_grad=True)\n",
    "        self.alpha_u = torch.tensor(_alpha_u, dtype=torch.float32, requires_grad=True)\n",
    "        self.beta_u = torch.tensor(_beta_u, dtype=torch.float32, requires_grad=True)\n",
    "        self.gamma = torch.tensor(_gamma, dtype=torch.float32, requires_grad=True)\n",
    "        self.theta = torch.tensor(_theta, dtype=torch.float32, requires_grad=True)\n",
    "        self._lambda = torch.tensor(__lambda, dtype=torch.float32, requires_grad=True)\n",
    "        self.d = torch.tensor(_d, dtype=torch.float32, requires_grad=True)\n",
    "        self.parameters=[self.alpha_r, self.beta_r, self.alpha_u, self.beta_u, self.gamma, self.theta, self._lambda, self.d]\n",
    "        self.T=T\n",
    "        self.load_data()\n",
    "    def call_sampler(self,n):\n",
    "        #remember to add .item() at final version\n",
    "        return torch.randn(10), torch.randn(10,self.T)\n",
    "        #The below is currently not working\n",
    "        params=(self.alpha_r.item(), self.beta_r.item(), self.d.item(), self.alpha_u.item(), self.beta_u.item(), self.gamma.item(), self.theta.item(), self._lambda.item())\n",
    "        sampler = TEST_SAMPLER(self.T, params)\n",
    "        samples, weights=sampler.sample(n, self.r)\n",
    "        return weights,samples.T\n",
    "    def load_data(self):\n",
    "        DG = Data_generator(0.2, 0.2, 6.0, 0.6, 0.4, 0.1, 0.02, 2.5)\n",
    "        self.r=torch.tensor(DG.gen_data(1, self.T))\n",
    "    def upd_param(self,lr=0.01):\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters:\n",
    "                param -= lr * param.grad #check + or -\n",
    "                param.grad.zero_()\n",
    "    def singularlikelihood(self,epsilon):\n",
    "        #calculate log-likelihood of each set of epsilon, (n,T) -> (n,)\n",
    "        return torch.randn(epsilon.shape[0])\n",
    "    def log_total(self):\n",
    "        n=10\n",
    "        weights,epsilon=self.call_sampler(n)\n",
    "        likelihood=self.singularlikelihood(epsilon)\n",
    "        normed=likelihood-torch.sum(likelihood).item()/n+torch.log(weights) #item for removing normalization from gradients\n",
    "        return torch.log(torch.sum(torch.exp(normed)))\n",
    "    def optimize(self,num_steps=10):\n",
    "        for _ in range(num_steps):\n",
    "            likelihood=self.log_total()\n",
    "            likelihood.backward()\n",
    "            self.upd_param(lr=0.01)\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "EM_sampler=EM(10,0.2, 0.2, 6.0, 0.6, 0.4, 0.1, 0.02, 2.5)\n",
    "EM_sampler.r.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
